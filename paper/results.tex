\section{Analyses \& Results}
\label{sec_analyses}

\subsection{RQ1: Comparison of tagged and untagged time-series}
Our principal research question is whether listening patterns for tagged content are consistent with the expectation that tags serve as memory cues. If this were to be the case, we would expect to see an increase in a user's listening rate to musical artists after the user has tagged them, under the assumption that a tag facilitates retrieval and increases the chances of a user listening to a tagged artist. 

Unfortunately, several factors combine to make such an analysis difficult.  First and foremost, the desired counterfactual of the untagged ``version'' of a particular tagged series, which would allow a direct testing of how tagging changes listening behavior, does not, of course, exist. We thus must utilize untagged time-series in a way that allows them to approximate what a true counterfactual might look like.  In searching for such samples, a second difficulty that arises is that listening rates for tagged time-series are much greater than for untagged time-series (the average number of total listens across time-series is 16.9 when untagged and 98.9 when tagged). While suggestive of the importance of tagging, this unbalance also suggests that controls must be instilled in both sample selection and statistical analysis to account for previous listening behavior prior to tagging. Finally, the actual point in time at which tags are expected to increase listening behavior for any given user is unknown. Thus, we must formulate our analysis to account for this uncertainty.  
%to understand how it changes behavior
%as it is theoretically possible that tagging may affect listening behavior as much three months after the tag has been placed as it does in the immediately following month.

%meticulously
To alleviate issues with the non-existence of a true counterfactual, we subselect from both the tagged and untagged series using the following formal procedure. We first temporally align the tagged and untagged time-series by selecting only those tagged time-series where the tag was applied in the month of peak listening and aligning them at that peak point, and then collecting a sample of untagged time-series also aligned at the peak of listening.  Where the peak was reached in multiple months, we chose one of these at random.
%Both tagged time-series are aligned so that they are centered on the month in which they were tagged.  If multiple tags were present, we selected the tag within the month which had the most corresponding scrobbles. While there is no analogue to this point in the untagged data, we can partially resolve the issue by noting that tagging is disproportionately likely (approximately 30\%, compared to 1.1\% if the tagging month were random) to occur in a user's \emph{peak}\footnote{The month in which they listen the most times overall} listening month for a given artist. This provides a basis for aligning the tagged and untagged time-series,

%while still admitting that there is likely a reasonable span in which tagging and past listening behavior have an effect on future actions
After aligning all tagged and untagged samples in this fashion, we further limited our analysis to a 13 month period extending from 6 months prior to the peak month to 6 months after the peak. This allows us to consider a variety of ways in which listening prior to the tag may affect future behavior. Finally, we further constrain our sampling to time-series with:
\begin{itemize}
\item more than 25 total listens; 
\item a peak in listening at least 6 months from the edges of our data collection period (i.e. ensuring that the period from 6 months before to 6 months after the peak does not extend beyond the limits of our data range); and
\item at least one listen 6 months prior to and after the peak (i.e. if the peak occurs in July, there should be at least one listen between January and June, and one between August and the following January).
\end{itemize}

Constraining our time-series in this manner, we are left with a total of 206,140 tagged time-series.  We then randomly sampled from the 4.1M untagged time-series an equal number meeting the same three criteria.  All results below have been verified with multiple random samplings of the untagged data.

In Figure~\ref{fig:taggedVsUntagged} we plot mean playcounts, with 95\% normal confidence intervals, for each month across all tagged and untagged time-series in the subsampled data. All values are normalized by the peak, and thus values at the peak month for both the tagged and untagged lines are unity. By visually comparing the line heights before and after the peak, Figure~\ref{fig:taggedVsUntagged} shows that the mean normalized listening rate increases in the months after the peak for both tagged and untagged time-series. However, we also see a small but reliable effect wherein tagged time-series show proportionally higher mean normalized listening rates after the peak month (in which the the tag was applied) as compared to untagged time-series. This is suggestive of an increase in listening as a result of tagging. 
\todo[inline]{Note from Peter: but there's also a small but reliable (presumably) *lower* rate of listening to the tagged artists *before* the peak--what's up with that?  In some sense this indicates that songs that ``catch on'' for a user more quickly (rise faster in listening from before the peak to the peak) are more likely to be tagged--so maybe we're just seeing that such ``faster-rising'' artists are also more likely to be listened to more later, and it's not a result of the tag?  I'd be happier if the lines before the peak overlapped exactly.... :)[From Jared: He makes a good point, but I'm not sure off the top of my head how best to address this, at least not concisely. I can add in something to the effect of what he says I guess.]}
\todo[inline]{Also from Peter, regarding Fig 1A: would be cleaner to change x-axis values to -6, -5, ... -1, 0, 1, 2, ... 6 and change label to ``Months before/after peak at 0'' or something like that.}
\todo[inline]{And regarding Fig. 1B (I think he's right on both issues with fig 1A/B): rather than plotting log listens (which are hard to think about), how about plotting listens, but on a log-log plot?}

While Figure~\ref{fig:taggedVsUntagged} thus gives evidence that supports our hypothesis, there are two important caveats to the data in the plot. First, because listening distributions are heavily skewed for any given month, the mean is not necessarily representative of the data. \todo[inline]{From Peter: not clear what listening distributions you're talking about (do you mean for a given user, across all the artists they listen to, or do you mean the number of listens across all these different user-trajectories for a given month? And I'd say ``not necessarily representative of any one user's data.'' (if that's what you mean). [Jared says: In retrospect. I'm not 100\% sure what you meant here either...]}Though qualitative plots of transformed variables proved to be similar, our further statistical analysis uses a log transformed version of the listening counts to account for these deviations. Second, while normalization controls for differences in listening counts between tagged and untagged data to some extent, we sought a method that would capture the effect of pre-peak listening behavior on post-peak listening.
%we would prefer a method that explicitly accounts for listening behavior prior to and including the peak month on future behavior.

To more robustly test our hypothesis, we therefore use a regression model relating post- and pre-peak listening behavior. Due to the volume of data we are dealing with, it was unreasonable make the assumption of linear dependence of the dependent variable on the independent variables. \todo[inline]{From Peter: not clear--why does having a lot of data mean that the relationship can't be linear?  That doesn't seem logical.  Reword/explain.  And also you could simplify the wording of the rest of the sentence to something like ``it was unreasonable to assume a linear relationship between the dependent and independent variables.''}We chose a Generalized Additive Model (GAM, \cite{hastie1990generalized}) using the R package mgcv \cite{wood2001mgcv}. Our dependent variable in the regression is the logarithm of the sum of all listens in the six months after a tag has been applied, to capture the possible effect of tagging over a wide temporal window.\footnote{Qualitatively, our results hold when testing listening for each individual month as well. Also of note is our choice of using the log of the dependent variable rather than a count-based regression model (e.g. a Negative Binomial regression). The model used here appeared to fit the data better based on a variety of statistical and visual goodness-of-fit tests.} Our independent variables are a binary indicator of whether or not the time-series has been tagged, as well seven continuous-valued predictors, one each for the logarithm of the sum of listens in the peak month and the six previous months.   

  \begin{figure}
    \subfloat[\label{fig:taggedVsUntagged}]{%
      \includegraphics[width=0.5\textwidth]{taggedVUntaggedSimple.png}
    }
    \hfill
    \subfloat[\label{fig:regression}]{%
      \includegraphics[width=0.45\textwidth]{taggedVUntaggedRegression.png}
    }
    \caption{Comparison of tagged and untagged listening time-series. Mean normalized playcount by month (a), and Regression results, with 95\% confidence interval (b).}
    \label{fig:regressionFigs}
  \end{figure}

The regression model, which explained approximately 30\% of the variance in the data (adjusted $R^{2}$), indicated that that listening rate parameters (smoothed using thin-plate regression splines) for all seven previous months had a significant effect on post-peak listening behavior. As we cannot show the form of this effect for all model variables at once, Figure~\ref{fig:regression} instead displays a similar model which considers only the effect of listening in the peak month on post-peak listening. As this plot suggests and the full model confirms, we can conclude that, controlling for all previous listening behavior, a tag increases the logarithm of post-peak listens by .147 (95\% CI = [.144,.150]). This indicates that the effect of a tag is associated with around 1.15 more listens over six months, on average, than if it were not to have been applied.   

\subsection{Tag analysis}
To examine if and how different tags are associated with increased future listening, we ran a regression analysis similar to that described above. Four important changes were made. First, we considered only tagged time-series.  Second, instead of a single tagged/untagged indicator, we included binary (present / not present) regressors for all unique tags that had at least 25 occurrences in our subsample. \todo[inline]{We need to justify this particular threshold...is it just that with less data we couldn't reliably estimate the regressors or...?}Third, due to the data-hungry nature of the GAM and the large number of additional variables introduced by utilizing all tags as unique predictors, we chose to only use listening in the peak month (for each tag) as the independent variables. This decision limited the computational difficulties associated with estimating a model of this size and did not appear to affect model fit substantially according to tests we ran on subsamples of the data. Finally, we eliminated the constraint that a tag must occur in the peak month of a time-series, as there is no meaningful comparison to be made with untagged data in this analysis. This allowed us to include additional tagged time-series. The dependent variable remained the log number of listens in the 6 months following tagging. If users had multiple tags for a particular artist, we again selected one tag randomly rather than include the same user artist combination twice in our analysis.

This expanded sample consisted of 600,701 tagged time-series, with 6,060 unique tags. After running the model, which explains approximately 17\% of the variance in the data (adjusted $R^{2}$), 321 unique tags were statistically significant predictors at $p <.001$. \todo[inline]{From Peter: Why this sig level? [Maybe it would make sense to just use 0.01? Then we could have more data for fig. 2...]}While we only have sufficient evidence to make claims about these 321 tags, qualitative examination of which tags are relatively strong predictors in the model proved informative.

The most telling observation is that commonly-used genre tags (e.g. ``pop'', ``jazz'', and ``hip-hop'') -- which are the most common tags overall in our full dataset -- tend to be weak, negative predictors of future listening. In contrast, relatively strong predictors (both positive and negative) appear to be comparatively obscure, possibly idiosyncratic tags (``cd collection'', ``mymusic'', ``purchased 09'').\footnote{For a full listing of the regression coefficients across all tags in the model, see \url{https://dl.dropboxusercontent.com/u/625604/papers/lorince.joseph.todd.2015.sbp.supplemental/regression_coefficients.txt}} \todo[inline]{Peter makes the good point that we can actually check if these are idiosyncratic by checking how many unique users use each of these tags. I can get those numbers to you, and maybe you can generate the same plot (or more points on this plot) showing the number of unique users for each tag?} To examine this trend quantitatively, we plot in Figure~\ref{fig:coefVsPopularity} global tag popularity (i.e. the total number of uses of a tag in our full dataset of approximately 50 million annotations) as a function of the tag's tag's impact on listening as indicated by its coefficient in the regression model, for all tags that had a statistically significant coefficient. \todo[inline]{From Peter: How are we to interpret these regression coefficients?  I.e., what does coef=1 mean in terms of the change in number of listens over the next six months?  In other words, were the ``strong'' tags having a sizable impact on listening?} The red bands marked the upper and lower limits of a bootstrapped 95\% confidence interval on the popularity of the 5,739 remaining tags that were \emph{not} significant in the regression model. The data suggest that the most popular tags are significant, weakly negative predictors of future listening, while relatively unpopular tags tend to have either strong positive or strong negative impacts on listening. Tags which were not significant in the model tend to be of moderate to high popularity.

  \begin{figure}[t]
	\centering
      \includegraphics[width=0.7\textwidth]{tagRegressionWithMoreData.png}
    \caption{Logarithm of tags' global popularity as a function of regression coefficient.}
    \label{fig:coefVsPopularity}
  \end{figure}
\todo[inline]{From Peter: Change y-axis to ``Global popularity''}
\todo[inline]{Big question: Doesn't it look like the ``weight'' of these points is on the negative side--i.e., more points indicating less listening after tagging?  But then how does that fit with the overall result of an *increase* in listening after tagging? [Jared says: Huh, that's totally true. If more tags seem to decreases listening, how is that the overall effect of tagging increases listening?]}

%These data suggest that, at least for the small number of tags about which we can make statistically meaningful claims, those that are globally popular and well-known have relatively little effect on future listening, and are generally associated with small \emph{decreases} in post-taggging listening rates. The tags that seem to ``matter'' (i.e. those that are relatively strong predictors of whether or not a user will listen to an artist after tagging it) are generally much less popular.